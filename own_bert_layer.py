# -*- coding: utf-8 -*-
"""own_bert_layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Fusgk8Wiip-4tL55ab93Zz4pV7vXk9g
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import tensorflow as tf
from tensorflow.keras import Model, layers, Input
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Embedding, Dot, Concatenate,Softmax, Layer
from tensorflow.keras.optimizers import Adam
# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import pandas as pd
import spacy
import numpy as np
sp = spacy.load('en_core_web_sm')
data = pd.read_csv("IMDB Dataset.csv").values
Y = data[:, 1]
Y = Y[(Y == "positive") | (Y == "negative")]
Y[Y == "positive"] = 1.
Y[Y == "negative"] = 0.
word_frequency = {}
def add_frequency(X, word_frequency):
    def add_frequency_word(word, word_frequency):
        try:
            word_frequency[word] += 1
        except:
            word_frequency[word] = 1
    many_changes = [add_frequency_word(word,  word_frequency) for sentence in X for word in sentence.split(" ")]
X = data[:, 0]
add_frequency(X, word_frequency)
vocabs = list(word_frequency.keys())
n_vocabs = len(vocabs)
index_sorted = np.argsort(np.array(list(word_frequency.values())))
print(index_sorted)
vocabs = np.array(vocabs)[index_sorted]
vocabs = vocabs.reshape(1, -1)
print(vocabs.shape)
ids = np.linspace(2, vocabs.shape[1] + 1, vocabs.shape[1]).reshape(1, -1)
ids = ids.astype(int)
print(ids.shape)
vocabs_ids = np.concatenate((vocabs, ids))
vocabs_ids = vocabs_ids.T
vocabs_ids = dict(vocabs_ids)
max_length = 128
def sentence_ids(sentence, vocabs_ids):
    return (np.array([vocabs_ids[word] for word in sentence.split(" ")]))
X = np.array([sentence_ids(sentence, vocabs_ids) for sentence in X])
from tensorflow.keras.preprocessing.sequence import pad_sequences
X = X
X = pad_sequences(X, maxlen = max_length - 1)
X = np.hstack((np.ones((X.shape[0],1)).astype(int), X))
X = X.astype(int)
Y = Y.astype(np.float32)
from keras.utils.np_utils import to_categorical
from tensorflow.keras import regularizers
Y = to_categorical(Y)
train_test = int(0.8 * X.shape[0])
X_test = X[train_test:]
Y_test = Y[train_test:]
X = X[:train_test]
Y = Y[:train_test]
class MultiHeadSelfAttention(Layer):
    def __init__(self, n_embedding, qk_hiddens, v_hidden,  **kwargs):
        super(MultiHeadSelfAttention, self).__init__(**kwargs)
        self.q_denses = [Dense(qk_hidden, kernel_regularizer = regularizers.l1_l2(l1 = 1e-4, l2 = 1e-3), bias_regularizer = regularizers.l1_l2(l1 = 1e-4, l2 = 1e-3), activity_regularizer = regularizers.l1_l2(l1 = 1e-4, l2 = 1e-3)) for qk_hidden in qk_hiddens]
        self.k_denses = [Dense(qk_hidden, kernel_regularizer = regularizers.l1_l2(l1 = 1e-4, l2 = 1e-3), bias_regularizer = regularizers.l1_l2(l1 = 1e-4, l2 = 1e-3), activity_regularizer = regularizers.l1_l2(l1 = 1e-4, l2 = 1e-3)) for qk_hidden in qk_hiddens]
        self.v_denses = [Dense(v_hidden, kernel_regularizer = regularizers.l1_l2(l1 = 1e-4, l2 = 1e-3), bias_regularizer = regularizers.l1_l2(l1 = 1e-4, l2 = 1e-3), activity_regularizer = regularizers.l1_l2(l1 = 1e-4, l2 = 1e-3)) for i in range(len(qk_hiddens))]
        self.softmax = Softmax(axis = -1)
        self.concatenate = Concatenate(axis = -1)
        self.linear = Dense(n_embedding)
    def call(self, embedding):
        multi_q = [q_dense(embedding) for q_dense in self.q_denses]
        multi_k = [k_dense(embedding) for k_dense in self.k_denses]
        multi_v = [v_dense(embedding) for v_dense in self.v_denses]
        multi_dot = [tf.matmul(self.softmax(tf.matmul(multi_q[i], tf.transpose(multi_k[i], [0, 2, 1])) / np.sqrt(multi_k[i].shape[-1])), multi_v[i])
                    for i in range(len(multi_q))]
        concat = self.concatenate(multi_dot)
        last_output = self.linear(concat)
        return last_output, embedding

class PositionalEncoding(Layer):
    def __init__(self, **kwargs):
        super(PositionalEncoding, self).__init__(**kwargs)
    def call(self, input):
        mau_so = np.full(input.shape[1:], np.linspace(0, input.shape[-1], input.shape[-1])/input.shape[-1])
        chan_le = (mau_so.astype(int) % 2)
        mau_so = np.power(1e4, mau_so)
        tu_so =  np.full(input.shape[1:], np.full((input.shape[-1], input.shape[-2]), np.linspace(0, input.shape[-2], input.shape[-2])).T)
        tu_mau = tu_so / mau_so
        tu_mau[chan_le == False] = np.sin(tu_mau[chan_le == False])
        tu_mau[chan_le] = np.cos(tu_mau[chan_le])
        return input + tf.convert_to_tensor(tu_mau, dtype=tf.float32)
class TransformerEncoder(Layer):
     def __init__(self , n_embedding, qk_hiddens, v_hidden, **kwargs):
        super(TransformerEncoder, self).__init__(**kwargs)
        self.multi_attention = MultiHeadSelfAttention(n_embedding, qk_hiddens, v_hidden)
        self.positional_encoding = PositionalEncoding()
        self.norm1 = BatchNormalization()
        self.norm2 = BatchNormalization()
        self.dense = Dense(n_embedding, kernel_regularizer = regularizers.l1_l2(l1 = 1e-3, l2 = 1e-3), bias_regularizer = regularizers.l1_l2(l1 = 1e-3, l2 = 1e-3), activity_regularizer = regularizers.l1_l2(l1 = 1e-3, l2 = 1e-3))
     def call(self, embedding_input):
        encoding = self.positional_encoding(embedding_input)
        encoding = embedding_input
        output_attention, input_attention = self.multi_attention(encoding)
        add_output1 = output_attention + input_attention
        norm_output1 = self.norm1(add_output1)
        dense_output = self.dense(norm_output1)
        add_output2 = norm_output1 + dense_output
        norm_output2 = self.norm2(add_output2)
        return norm_output2
class Bert(Layer):
    def __init__(self , n_transformer, n_embedding, qk_hiddens, v_hidden, cls_hidden, n_classes, dropout_ratio, **kwargs):
        super(Bert, self).__init__(**kwargs)
        self.list_transformer = [TransformerEncoder(n_embedding, qk_hiddens, v_hidden) for i in range(n_transformer)]
        self.dropout = Dropout(dropout_ratio)
        self.cls_dense = Dense(cls_hidden, activation = "relu", kernel_regularizer = regularizers.l1_l2(l1 = 1e-3, l2 = 1e-3), bias_regularizer = regularizers.l1_l2(l1 = 1e-3, l2 = 1e-3), activity_regularizer = regularizers.l1_l2(l1 = 1e-3, l2 = 1e-3))
        self.dropout2 = Dropout(dropout_ratio)
        self.cls_output = Dense(n_classes, activation = "softmax")
    def call(self, embedding):
        output_transformer = embedding
        for transformer in self.list_transformer:
            output_transformer = transformer(output_transformer)
        get_cls = output_transformer[:, 0, :]
        dropout_cls = self.dropout(get_cls)
        dense_output = self.cls_dense(dropout_cls)
        dropout_cls2 = self.dropout(dense_output)
        output = self.cls_output(dropout_cls2)
        return output

n_embedding = 300
qk_hiddens = [128, 256, 512]
v_hiddens = 32
n_classes = 2
n_transformers = 5
n_dropout = 0.4
cls_hidden = 256
X_position = np.full(X.shape, np.linspace(0, max_length - 1, max_length)).astype(int)
word_input = Input((max_length))
position_input = Input((max_length))
word_embedding = Embedding(n_vocabs + 2, n_embedding, input_length = max_length)(word_input)
position_embedding = Embedding(max_length, n_embedding, input_length = max_length)(position_input)
output = Bert(n_transformers, n_embedding, qk_hiddens, v_hiddens, cls_hidden, n_classes, n_dropout)(word_embedding + position_embedding)
model = Model([word_input, position_input], output)
optimizer = Adam(learning_rate = 1e-3)
model.compile(optimizer = optimizer, loss = "categorical_crossentropy", metrics = ["acc"])
batch_size = 32
epochs = 30
with tf.device("/gpu:0"):
    model.fit([X, X_position], Y, batch_size = batch_size, epochs = epochs, validation_split = 0.1, shuffle = False)
X_position_test = np.full(X_test.shape, np.linspace(0, max_length - 1, max_length)).astype(int)
model.evaluate([X_test, X_position_test], Y_test)